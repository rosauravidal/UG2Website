<!DOCTYPE html>
<html lang="en-US" dir="ltr">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <!--  
    Document Title
    =============================================
  -->
  <title>UG2+ Challenge</title>
    <!--  
    Favicons
    =============================================
  -->
  <link rel="apple-touch-icon" sizes="57x57" href="assets/images/favicons/apple-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="assets/images/favicons/apple-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="assets/images/favicons/apple-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="assets/images/favicons/apple-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="assets/images/favicons/apple-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="assets/images/favicons/apple-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="assets/images/favicons/apple-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="assets/images/favicons/apple-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="assets/images/favicons/apple-icon-180x180.png">
  <link rel="icon" type="image/png" sizes="192x192" href="assets/images/favicons/android-icon-192x192.png">
  <link rel="icon" type="image/png" sizes="32x32" href="assets/images/favicons/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="96x96" href="assets/images/favicons/favicon-96x96.png">
  <link rel="icon" type="image/png" sizes="16x16" href="assets/images/favicons/favicon-16x16.png">
  <link rel="manifest" href="/manifest.json">
  <meta name="msapplication-TileColor" content="#ffffff">
  <meta name="msapplication-TileImage" content="assets/images/favicons/ms-icon-144x144.png">
  <meta name="theme-color" content="#ffffff">
    <!--  
    Stylesheets
    =============================================
    
  -->
  <!-- Default stylesheets-->
  <link href="assets/lib/bootstrap/dist/css/bootstrap.min.css" rel="stylesheet">
  <!-- Template specific stylesheets-->
  <link href="https://fonts.googleapis.com/css?family=Roboto+Condensed:400,700" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Volkhov:400i" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,600,700,800" rel="stylesheet">
  <link href="assets/lib/animate.css/animate.css" rel="stylesheet">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.3/css/all.css" integrity="sha384-UHRtZLI+pbxtHCWp1t77Bi1L4ZtiqrqD80Kn4Z8NTSRyMA2Fd33n5dQ8lWUE00s/" crossorigin="anonymous">
  <link href="assets/lib/et-line-font/et-line-font.css" rel="stylesheet">
  <link href="assets/lib/flexslider/flexslider.css" rel="stylesheet">
  <link href="assets/lib/owl.carousel/dist/assets/owl.carousel.min.css" rel="stylesheet">
  <link href="assets/lib/owl.carousel/dist/assets/owl.theme.default.min.css" rel="stylesheet">
  <link href="assets/lib/magnific-popup/dist/magnific-popup.css" rel="stylesheet">
  <link href="assets/lib/simple-text-rotator/simpletextrotator.css" rel="stylesheet">
  <!-- Main stylesheet and color file-->
  <link href="assets/css/style.css" rel="stylesheet">
  <link id="color-scheme" href="assets/css/colors/default.css" rel="stylesheet">
</head>
<body data-spy="scroll" data-target=".onpage-navigation" data-offset="60">
  <a id="ddmenuLink" href="menu_transparent.html">Menu</a>
  <main>
    <div class="page-loader">
      <div class="loader">Loading...</div>
    </div>

    <div class="main">
      <section class="module bg-dark-30 portfolio-page-header" data-background="assets/images/classification.jpg" style="padding: 30px 0;">
        <div class="container">
          <div class="row" style="padding-top: 40px">
            <div class="col-sm-6 col-sm-offset-3">
              <h2 class="module-title font-alt" style="margin: 0 0 20px">The UG<sup>2</sup>+ Datasets</h2>
              <h3 class="module-subtitle font-serif" style="margin: 0 0 20px"><a href="#track1" class="section-scroll btn btn-border-w btn-round">Track 1</a>  <a href="#track2" class="section-scroll btn btn-border-w btn-round">Track 2</a> </h3>

            </div>
          </div>
        </div>
      </section>

      <section class="module" style="padding: 30px 0;" id='track1'>
        <div class="container">
          <div class="row">
            <h3 class="module-title font-alt align-left" style="margin-bottom: 40px"><span class="fas fa-helicopter"></span> Track 1: Video object classification and detection from unconstrained mobility platforms</h3>
            <div class="col-sm-12 col-md-12 col-lg-12">
              <div class="work-details">
                <h4 class="work-details-title font-alt">About the Dataset</h4>
                <p>UG<sup>2</sup> contains three difficult real-world scenarios: uncontrolled videos taken by UAVs and manned gliders, as well as controlled videos taken on the ground. Over 150,000 annotated frames for hundreds of ImageNet classes are available.</p>
                <h5 class="work-details-title font-alt">Training</h5>
                <p>The data we release can be used for training and validation purposes. With respect to restoration and enhancement approaches that must be trained, we encourage a cross-dataset protocol where some annotated training data could come from outside UG<sup>2</sup>. Additionally we encourage participants to make use of their own data or data from other sources for training. However, un-annotated videos for additional validation purposes and parameter tuning are provided.</p>
                <ul>
                  <li><strong>Paper: </strong><span class="font-serif"><a href="https://arxiv.org/abs/1710.02909" target="_blank">ArXiv</a></span>
                  </li>
                  <li><strong>Release Date: </strong><span class="font-serif">October, 2017</span>
                  </li>
                  <li><strong>Download: </strong><span class="font-serif"><a href="https://goo.gl/AjA6En" target="_blank">https://goo.gl/AjA6En</a></span>
                  </li>
                </ul>
                <h4 class="work-details-title font-alt" style="padding-top: 40px">Data Annotations</h4>

                <div class="module-subtitle font-serif align-left" style="margin-bottom: 20px">
                  The dataset contains annotations for 162,136 object-level annotated images.  Bounding boxes establishing object regions were manually annotated using the <a href="https://github.com/cvondrick/vatic">VATIC Video Annotation Tool</a>, we provide the VATIC annotation files for every annotated video in the dataset.
                </div>
                <p>Each annotation file follows the annotation structure provided by VATIC. Each line contains one object annotation which is defined by 10 columns. The definition of each column is as follows:</p>
                <ol>
                  <li><b>Track ID.</b> All rows with the same ID belong to the same path of the same object through different video frames.</li>
                  <li><b>xmin.</b> The top left x-coordinate of the bounding box.</li>
                  <li><b>ymin.</b> The top left y-coordinate of the bounding box. 
                  xmax. The bottom right x-coordinate of the bounding box.</li>
                  <li><b>ymax.</b> The bottom right y-coordinate of the bounding box.</li>
                  <li><b>frame.</b> The frame that this annotation represents.</li>
                  <li><b>lost.</b> If 1, the annotation is outside of the view screen. In this case we did not extract any cropped region.</li>
                  <li><b>occluded.</b> If 1, the annotation is occluded. In this case we did not extract any cropped region.</li>
                  <li><b>generated.</b> If 1, the annotation was automatically interpolated. 
                  label. The class for this annotation, enclosed in quotation marks.</li>
                </ol>
              </div>
            </div>
          </div>
        </div>
      </section>

      <hr>

      <section class="module" style="padding: 30px 0;" id='track2'>
        <div class="container">
          <div class="row">
            <h3 class="module-title font-alt align-left" style="margin-bottom: 40px"><span class="fas fa-cloud-sun-rain"></span> Track 2: Object Detection in Poor Visibility Environments</h3>
            <div class="col-sm-12 col-md-12 col-lg-12">
              <div class="work-details">
                <h4 class="work-details-title font-alt">About the Dataset</h4>
                <p>We structure this track into three sub-challenges. Each challenge features a different poor-visibility outdoor condition, and diverse training protocols (paired versus unpaired images, annotated versus unannotated, etc.).</p>
                <h5 class="work-details-title font-alt">Training</h5>
                <p>In all three sub-challenges, the participant teams are allowed to use external training data that are not mentioned above, including self-synthesized or self-collected data; but they must state so in their submissions.</p>

                <h4 class="work-details-title font-alt">Sub-Challenge 2.1: (Semi-)Supervised Object Detection in the Haze</h4>
                <p>We provide a set of 4,322 real-world hazy images collected from traffic surveillance, all labeled with object bounding boxes and categories (car, bus, bicycle, motorcycle, pedestrian), as the main training and/or validation sets. We also release another set of 4,807 unannotated real-world hazy images collected from the same sources (and containing the same classes of traffic objects, though not annotated), which might be used at the participantsâ€™ discretization. There will be a hold-out testing set of 3,000 real-world hazy images, with the same classes of objected annotated.</p>
                <ul style="padding-left: 20px">
                  <li><strong>Paper: </strong><span class="font-serif"><a href="https://arxiv.org/abs/1712.04143" target="_blank">ArXiv</a></span>
                  </li>
                  <li><strong>Release Date: </strong><span class="font-serif">December, 2017</span>
                  </li>
                  <li><strong>Download: </strong><span class="font-serif"><a href="https://sites.google.com/site/boyilics/website-builder/reside" target="_blank">Benchmarking Single Image Dehazing and Beyond</a></span>
                  </li>
                </ul>

                <h4 class="work-details-title font-alt" style="padding-top: 30px">Sub-Challenge 2.2: (Semi-)Supervised Face Detection in the Low Light Condition</h4>
                <p>We provide 6,000 real-world low light images captured during the nighttime, at teaching buildings, streets, bridges, overpasses, parks etc., all labeled with bounding boxes for of human face, as the main training and/or validation sets. We also provide 9,000 unlabeled low-light images collected from the same setting. Additionally, we provided a unique set of 1,000 paired low-light / normal-light images captured in controllable real lighting conditions (but unnecessarily containing faces), which can be used as parts of the training data at the participantsâ€™ discretization. There will be a hold-out testing set of 4,000 low-light images, with human face bounding boxes annotated.</p>
                <ul style="padding-left: 20px">
                  <li><strong>Paper: </strong><span class="font-serif"><a href="https://arxiv.org/abs/1808.04560" target="_blank">ArXiv</a></span>
                  </li>
                  <li><strong>Release Date: </strong><span class="font-serif">August, 2018</span>
                  </li>
                  <li><strong>Download: </strong><span class="font-serif"><a href="https://daooshee.github.io/BMVC2018website/" target="_blank">LOw Light paired dataset (LOL)</a></span>
                  </li>
                </ul>

                <h4 class="work-details-title font-alt" style="padding-top: 30px">Sub-Challenge 2.3: Zero-Shot Object Detection with Raindrop Occlusions</h4>
                <p>We provide 1,010 pairs of raindrop images and corresponding clean ground-truths (collected through physical simulations), as the training and/or validation sets. Different from Sub-Challenges 2.1 and 2.2, no semantic annotation will be available on training/validation images. A hold-out testing set of 2, 496 real-world raindrop images are collected from high-resolution driving videos, in diverse real locations and scenes during multiple drives. We label bounding boxes for selected traffic object categories: car, person, bus, bicycle, and motorcycle.</p>
                <ul style="padding-left: 20px">
                  <li><strong>Paper: </strong><span class="font-serif"><a href="https://arxiv.org/abs/1711.10098" target="_blank">ArXiv</a></span>
                  </li>
                  <li><strong>Release Date: </strong><span class="font-serif">May, 2018</span>
                  </li>
                  <li><strong>Download: </strong><span class="font-serif"><a href="https://github.com/rui1996/DeRaindrop" target="_blank">Raindrop Dataset</a></span>
                  </li>
                </ul>


              </div>
            </div>
          </div>
        </div>
      </section>

      <a id="ddfooterLink" href="footer.html">Footer</a>
      <div class="scroll-up"><a href="#totop"><i class="fa fa-angle-double-up"></i></a></div>
    </main>
    <!--  
    JavaScripts
    =============================================
    -->
    <script src="assets/lib/jquery/dist/jquery.js"></script>
    <script src="assets/lib/bootstrap/dist/js/bootstrap.min.js"></script>
    <script src="assets/lib/wow/dist/wow.js"></script>
    <script src="assets/lib/jquery.mb.ytplayer/dist/jquery.mb.YTPlayer.js"></script>
    <script src="assets/lib/isotope/dist/isotope.pkgd.js"></script>
    <script src="assets/lib/imagesloaded/imagesloaded.pkgd.js"></script>
    <!-- <script src="assets/lib/flexslider/jquery.flexslider.js"></script> -->
    <script src="assets/lib/owl.carousel/dist/owl.carousel.min.js"></script>
    <!-- <script src="assets/lib/smoothscroll.js"></script> -->
    <script src="assets/lib/magnific-popup/dist/jquery.magnific-popup.js"></script>
    <script src="assets/lib/simple-text-rotator/jquery.simple-text-rotator.min.js"></script>
    <script src="assets/js/plugins.js"></script>
    <script src="assets/js/main.js"></script>
    <script src="assets/js/ddmenu.js" type="text/javascript"></script>
    <script src="assets/js/ddfooter.js" type="text/javascript"></script>
  </body>
</html>